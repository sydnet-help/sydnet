---
title: "SOCI832: Introduction 1: Introduction to Statistics"

lastmod: 2019-05-10T00:00:00.000Z

draft: false
type: docs
maths: true

output:
  blogdown::html_page:
    toc: true

linktitle: "1.2 Intro: Statistics"
menu:
  docs:
    parent: SOCI832
    weight: 120
---

<style>
table, th, td, tr, tbody {
  border-top: 1px solid LightGray;
    border-left: 0px solid LightGray;  
    border-right: 0px solid LightGray;  
    border-bottom: 1px solid LightGray;
    font-size: 0.7rem;
}
</style>

<table>  <tr>  <td>

# Reading

[Field, A., Miles, J., and Field, Z. (2012). *Discovering statistics using R*. Sage publications.](http://a.co/2jVI7VH)

* Chapter 2: Everything you ever wanted to know about statistics (well, sort of)

</td>  </tr>  </table>

<table>  <tr>  <td>
# Concepts 

<b style="padding-left: 2em">Descriptive statistics<br></b>
<b style="padding-left: 4em">Mean/Median/Mode (Central tendency)<br></b>
<b style="padding-left: 4em">Standard deviation/Interquartile range (Variation/dispersion)<br></b>
<b style="padding-left: 4em">Minimum/Maximum<br></b>
<b style="padding-left: 4em">Percentile<br></b>
<b style="padding-left: 4em">N (number of non-missing cases)<br></b>
<b style="padding-left: 2em">Inferential statistics<br></b>
<b style="padding-left: 4em">Population<br></b>
<b style="padding-left: 6em">Population parameter<br></b>
<b style="padding-left: 4em">Sample<br></b>
<b style="padding-left: 6em">Sample statistic<br></b>
<b style="padding-left: 4em">Hypothesis testing<br></b>
<b style="padding-left: 6em">Null hypothesis<br></b>
<b style="padding-left: 6em">Sampling distribution<br></b>
<b style="padding-left: 6em">Central limit theorm<br></b>
<b style="padding-left: 6em">Standard error<br></b>
<b style="padding-left: 6em">Confidence interval<br></b>
<b style="padding-left: 6em">p-value<br></b>
<b style="padding-left: 6em">Effect size<br></b>
<b style="padding-left: 4em">Bivariate inferential statistics<br></b>
<b style="padding-left: 6em">Correlation, comparison of means, chi-squared<br></b>
<b style="padding-left: 4em">Multivariate inferential statistics<br></b>
<b style="padding-left: 6em">Linear and logistic regression<br></b>
<b style="padding-left: 4em">Dimension reduction and finding categories<br></b>
<b style="padding-left: 6em">Factor analysis, Cluster analysis<br></b>


</td>  </tr>  </table>

<table>   <tr>  <td>

# Summary

[insert summary]


</td>   </tr>  </table>

# 1. Descriptive statistics: two numbers that are as good as a million

In this course I assume you are allergic to maths. This brings us all to the same level, and it also makes sure all of us try to build on solid foundations. 

When we use maths for social sciences, we generally use it in one of two ways: to generate **descriptive statistics** - which allow us to summarise a large amount of data in a few numbers - or to generate **inferential statistics** - which allow us to draw conclusions about the wider world based on a tiny sample of that world. 

**Descriptive statistics** allow us to summarise a group we can actually see, touch, and/or count. It helps us turn, for example, the incomes of 23 million Australian's who did the 2016 census into a single number - the median personal weekly personal income of people aged 15 years and over: [$662.](https://quickstats.censusdata.abs.gov.au/census_services/getproduct/census/2016/quickstat/036)

## 1.1 Mean/Median/Mode: The centre

One type of descriptive statistics try to summarise the "centre" of a dataset (or more correctly, the centre of the values of a variable). The most common example of this is the **average** - formally called **mean** - of a set of numbers. 

Other measures of the central tendency are the **median**, and the **mode**. Box 1 provides definitions, uses, and examples. 

All of these measures of the central tendency have a common characteristic: they are trying to find **one number that can represent them all.** It's like running a talent contest to find the 'most ordinary, normal, typical Australian', except in this case you are trying to find the 'most ordinary, normal, typical number in this set'.

<table>   <tr>  <td>

<p style="font-size: 0.8rem;"> **Box 1: Definitions, uses, and examples of mean, median, and mode** </p>

**Mean** (the arithmetic mean): The average. The sum of all cases divided by the number of cases. <br>
The mean is good for summarising the centre of a set of numbers that are **normally distributed** (we will learn about this later). For example, height is normally distributed, so the mean is a good summary measure for a group of people's heights.

**Median**: The middle point in a set of numbers. If we order a set of numbers from highest to lowest, and find the number in the middle, this is the median. If two numbers are in the middle (there is an even number of cases), then the median is the average to the two numbers. <br>
The median is good for summarising the centre of a set of numbers that is **skewed** (i.e. there are a large number of small values, or a large number of large values). For example, income is highly skewed - i.e. unequally distributed, with a large number of poor people, and a few people who are very very rich - and the median is a better measure of this. In Australia the mean income is around <span>&#36;</span>80,000/year, while the median closer to <span>&#36;</span>50,000/year. 

**Mode**: The most common number in a set. <br>
The mode is good for summarising the centre of a set of numbers that are **not ordered** (i.e. categorical/nominal). For example, if we have a list of 40,000 students and Macquarie Uni, and a variable which is the name of their degree, it doesn't make sense to talk about the mean or median of the 'degree'. However the 'mode' makes sense. In every day language, we would say the most common degree at Macquarie University is an Arts Degree. Another way to say this, is the 'mode' degree is an Arts Degree. 

</td>   </tr>  </table>

## 1.2 Standard deviation/Interquartile range: Giving variation a number

We know from life, however, that the 'most ordinary, normal, typical' single example of anything is a gross simplication. 

All sets have variety, and the second major type of descriptive statistic tries to capture this variety, and do it in just one number. 

### Standard deviation
The most common measure of variety - or technically 'variability', 'variation', or 'dispersion' - in a set of numbers is probably the **standard deviation**. 

Box 2 provides an explanation of standard deviation - as equation, and also in everyday language. 

Intiutively - not strickly - you can think of **standard deviation** as the average amount - as an absolute value (so negative become positive, and positive stay positive) - that each person's value for a variable 'misses' (deviates) from the mean of the variable. 

If the standard deviation is close to zero, then virtually everyone in the dataset has the same value for the variable. 

If the standard deviation is large, then you expect that - on average - any particular person you pick from the dataset is going to be quite distant from (greater or lesser than) the mean. 

### Normal distribution

### Interquartile range
While standard deviation is a good measure of variablity for 

<table>   <tr>  <td>

<p style="font-size: 0.8rem;"> **Box 2: Standard Deviation: As an equation and in everyday language** </p>

$\begin{aligned}
\text{Standard Deviation of variable x} = &\sqrt{\frac{\sum_{i=1}^n (x_i - \bar x)^2}{N}}\\
\\
\text{Where:} \\
x_i = &\text{value of x for each individual i} \\
\bar x = &\text{ mean of x} \\
N,n = &\text{ number of observations} \\
\sum_{i=1}^n = &\text{sum for all observations from 1 to n} \\
\end{aligned}$

While this equation looks scary to many people, I think the best way to break it down - conceptually, and by analogy, not strict mathematically - is to think about this as having just three main parts:

* **Variation from mean**: First, it basically asks how much, each value of a variable (e.g. the age of each particular student in a class) varies from the mean for the whole set (e.g. the average age of the class). This is represented in the equation as: $$(x_i - \bar x)$$
* **Averaged over all individuals**: Second, it takes the average (mean) of this across all observations in the dataset (e.g. all students in the class). This is represented by the sum of (the funny E) divided by N: $$\frac{\sum_{i=1}^n \text{variation from mean} }{N}$$
* **Punishing (counting) big variations more than small ones**: Third, it actually squares the deviation from the mean - which has the effect of 'punishing' (i.e. inflating or more heavily weighting) larger deviations from the mean more. It then takes the square root of the whole thing, basically to counter act the effect of the squaring, and make it all measured in the same 'units' as  the mean. 

BUT, the take away from all this is: **the standard deviation is - more or less - the average amount all values of a variable vary from the mean.**

For example, you have two classes of 100 students each, and the average age (mean) of each class is 22, but the standard deviation of the first class is zero, and standard deviation of the second class is one.  

Based on this standard deviation, you would know that - **roughly** and **on average** - if you picked any, say, six students from each class:

* the six students from the class with zero standard deviation would all be aged 22 (i.e. ages 22, 22, 22, 22, 22, 22), while 
* the six students from class with one year standard deviation, would be comprised of students who - on average - deviate from the mean age by one year (e.g. ages 20, 21, 22, 22, 23, 24). 

Notice that both groups have the same **mean** (22), but the second class is much more 'dispersed' and 'varied': the higher the standard deviation, the more 'spread out' the ages in the class are. 

</td>   </tr>  </table>


## 1.3 Minimum/Maximum: The bounds of our data

## 1.4 Percentile: For categories

## 1.5 N (number of non-missing cases): Did people actually answer this question?

# 2. Inferential statistics: Drawing conclusions about the big bad world

## 2.1 Population: The big bad world, hidden behind a curtain

### 2.1.1 Population parameter: The number we will never know

## 2.2 Sample: The mini world we can really see and touch

### 2.2.1 Sample statistic: Our approximation of the real world

## 2.3 Hypothesis testing: How do I know if I am wrong?

### 2.3.1 Null hypothesis: The hypothesis that nothing is happening

### 2.3.2 Sampling distribution: If i could do this survey 1,000 times...

### 2.3.3 Central limit theorm: If you roll dice infinite times you always get 3.5 (on average)

### 2.3.4 Standard error: If I repeated this survey 1,000 times... the answer would vary this much (holds up hands pretending to have caught a big fish).

### 2.3.5 Confidence interval: The population parameter is somewhere between here and here

### 2.3.6 p-value: the chance I'm wrong. The chance nothing is happening

### 2.3.7 Effect size: Does it really matter? How much? 

## 2.4 Bivariate inferential statistics: When X goes up, does Y go up?

### 2.4.1 Correlation, comparison of means, chi-squared

## 2.5 Multivariate inferential statistics: "But what about Z?" Modelling many 'causes'.

### 2.5.1 Linear and logistic regression

## 2.6 Dimension reduction: Finding categories in your data. 

### 2.6.1 Factor analysis, Cluster analysis


# Additional Resources

[Reference](link)

[Reference](link)

<center>
Last updated on _`r format(Sys.time(), '%d %B, %Y')`_ by _Dr Nicholas Harrigan_ [(nicholas.harrigan@mq.edu.au)](mailto:nicholas.harrigan@mq.edu.au)
</center>
