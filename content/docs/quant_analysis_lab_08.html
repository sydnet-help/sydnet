---
title: "Lab 8: Logistic Regression"

lastmod: 2018-12-30T00:00:00

draft: false
type: docs
maths: true	

output:
  blogdown::html_page:
    toc: true

linktitle: "SPSS 8: Logistic Regression"
menu:
   docs:
      parent: SOC224 SPSS Intro
      weight: 75

---


<div id="TOC">
<ul>
<li><a href="#reading">Reading</a></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#running-a-logistic-regression">Running a logistic regression</a></li>
</ul>
</div>

<style> img { padding:1px; border:1px solid #021a40; } </style>
<div id="reading" class="section level1">
<h1>Reading</h1>
<p>‘Chapter 19: Logistic Regression’ in <a href="https://www.amazon.com/Discovering-Statistics-Using-IBM-SPSS/dp/1446249182">Andy Field, 2017. Discovering Statistics Using IBM SPSS Statistics. Sage.</a></p>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<p>We generally use logistic regressions when we have</p>
<ol style="list-style-type: decimal">
<li>a dependent variable that is binary (i.e. it has only two values, 0 and 1), and</li>
<li>we want to test the impact of (and/or control for) multiple independent variables.</li>
</ol>
<p>The process of running a regression in SPSS is basically the same as for linear regressions, with options for forced entry, hierarchical, and stepwise methods.</p>
<p>When you read your results from a regression, you read the same two columns as in a linear regression (B and sig.).</p>
<p>Significance (sig.) is read the same as for a linear regression.</p>
<p>Because of the binary nature of the dependent variable in a logistic regression, the B is not so straightforward to interpret.</p>
<p>B is basically a number which represents the impact of the independent variable on the probably of the dependent variable being 1.</p>
<p>For this course, we are only going to interpret the significance, and direction (positive or negative) of B. We won’t interpret the magnitude. So for significant (&lt; 0.05) B coefficients we say that independent variables with positive B (greater than 0) increase the likelihood of the dependent variable being 1 (and the reverse for negative B values).</p>
<p>We can also view the various different R-square values for the model, which is approximately the same meaning as that for a linear regression.</p>
</div>
<div id="running-a-logistic-regression" class="section level1">
<h1>Running a logistic regression</h1>
<p>I’m just going to illustrate logistic regressions with FORCED ENTRY, because the procedure for hierarchical and stepwise methods is the same as that for linear regressions.</p>
<ol style="list-style-type: decimal">
<li>Select Analyze &gt; Regression &gt; Binary Logistic…</li>
</ol>
<p><img src="/img/htad_lesson_07_image_01.jpg" width="400" style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Select the dependent variable and put it in the top box</p></li>
<li><p>Select the independent variables and put these in the ‘Independent(s)’ box.</p></li>
</ol>
<p><img src="/img/htad_lesson_07_image_02.jpg" width="400" style="display: block; margin: auto;" /></p>
<ol start="3" style="list-style-type: decimal">
<li><p>Press OK. The regression will run and the output screen will appear</p></li>
<li><p>You can then interpret the coefficients of the regression.</p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>Look in the ‘Sig.’ column for the p-values</p></li>
<li><p>If the p-value &lt; 0.05 then the independent variable has a significant impact on the dependent variable</p></li>
<li><p>For the significant variables, we then read the B values (coefficients), which are the effect of a one unit increase of the independent variable on the dependent variable.</p></li>
<li><p>The problem with Logistic Regressions is that the independent variables DO NOT HAVE A LINEAR effect on the dependent variable. This makes the B values difficult to interpret. For this course we are not going to interpret the meaning of B values, except to ask (1) are they statistically significant; and (2) are they positive or negative, i.e. does the independent variable increase or decrease the likelihood of the dependent variable being 1 (rather than 0)</p></li>
<li><p>In this regression we can see that Gender is not statistically significant (p = 0.146)</p></li>
<li><p>Age is statistically significant. The older a person is, the less likely they are a virgin (B = -0.224)</p></li>
<li><p>Religiosity is statistically significant. The more religious a person, the more likely they are a virgin (B = 0.249)</p></li>
</ol>
<p><img src="/img/htad_lesson_07_image_03.jpg" width="400" style="display: block; margin: auto;" /></p>
<ol start="5" style="list-style-type: decimal">
<li>You can find an R-square value for the regression model, in the SPSS output:</li>
</ol>
<p><img src="/img/htad_lesson_07_image_04.jpg" width="400" style="display: block; margin: auto;" /></p>
<p>You can choose to interpret either the Cox &amp; Snell R Square, or the Nagelkerke R Square. These basically say that between 9% and 12% of the explained variance in virginity is explained by our three variables (gender, age, and religiosity).</p>
</div>
